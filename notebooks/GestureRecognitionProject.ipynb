{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "#experiment with the batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize images\n",
    "def resize_image(image, size):\n",
    "    image = image.astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize(size, Image.ANTIALIAS)\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function\n",
    "def generator(source_path, folder_list, batch_size, augment=False):\n",
    "    print('Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = np.arange(0, 40, 3)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list) // batch_size\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size, len(img_idx), 50, 50, 3))\n",
    "            batch_labels = np.zeros((batch_size, 5))\n",
    "            for folder in range(batch_size):\n",
    "                folder_path = source_path + '/' + t[folder + (batch * batch_size)].split(';')[0]\n",
    "                imgs = os.listdir(folder_path)\n",
    "                \n",
    "                # Adjusting img_idx if it exceeds the number of images available\n",
    "                valid_idx = [i for i in img_idx if i < len(imgs)]\n",
    "                \n",
    "                for idx, item in enumerate(valid_idx):\n",
    "                    image_path = folder_path + '/' + imgs[item]\n",
    "                    image = imread(image_path).astype(np.float32)\n",
    "                    image = resize_image(image, (50, 50))\n",
    "                    batch_data[folder, idx, :, :, :] = image / 255.0  # Normalize the image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            if augment:\n",
    "                datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "                yield datagen.flow(batch_data, batch_labels, batch_size=batch_size).__next__()\n",
    "            else:\n",
    "                yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/datasets/Project_data/train'\n",
    "val_path = '/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "# choose the number of epochs\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Function to build Conv3D model\n",
    "def build_conv3d_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(16, (3, 3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimiser = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Baseline Conv3D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment 1: Baseline Conv3D Model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 14, 50, 50, 16)    1312      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 50, 50, 16)   64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 7, 25, 25, 16)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 25, 25, 16)     0         \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 25, 25, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 12, 12, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3, 12, 12, 32)     0         \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 12, 12, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 6, 6, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 6, 6, 64)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,253,701\n",
      "Trainable params: 1,253,477\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 12:56:53.668111: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-09-02 12:56:53.668179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:3f:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Baseline Conv3D Model\n",
    "print(\"Running Experiment 1: Baseline Conv3D Model\")\n",
    "\n",
    "input_shape = (len(np.arange(0, 40, 3)), 50, 50, 3)\n",
    "model = build_conv3d_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.02)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_664/385748685.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 12:56:56.307162: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 12.2888 - categorical_accuracy: 0.3720Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2024-09-0212_56_52.893161/model-00001-12.28880-0.37202-9.46373-0.21875.h5\n",
      "21/21 [==============================] - 27s 1s/step - loss: 12.2888 - categorical_accuracy: 0.3720 - val_loss: 9.4637 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 9.0787 - categorical_accuracy: 0.4509\n",
      "Epoch 00002: saving model to model_init_2024-09-0212_56_52.893161/model-00002-9.07872-0.45089-10.17120-0.15625.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 9.0787 - categorical_accuracy: 0.4509 - val_loss: 10.1712 - val_categorical_accuracy: 0.1562 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 8.0306 - categorical_accuracy: 0.5625\n",
      "Epoch 00003: saving model to model_init_2024-09-0212_56_52.893161/model-00003-8.03055-0.56250-11.72482-0.18750.h5\n",
      "21/21 [==============================] - 24s 1s/step - loss: 8.0306 - categorical_accuracy: 0.5625 - val_loss: 11.7248 - val_categorical_accuracy: 0.1875 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 7.2313 - categorical_accuracy: 0.6027\n",
      "Epoch 00004: saving model to model_init_2024-09-0212_56_52.893161/model-00004-7.23128-0.60268-11.95828-0.15625.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 7.2313 - categorical_accuracy: 0.6027 - val_loss: 11.9583 - val_categorical_accuracy: 0.1562 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.4139 - categorical_accuracy: 0.6756\n",
      "Epoch 00005: saving model to model_init_2024-09-0212_56_52.893161/model-00005-6.41387-0.67560-12.29656-0.15625.h5\n",
      "21/21 [==============================] - 24s 1s/step - loss: 6.4139 - categorical_accuracy: 0.6756 - val_loss: 12.2966 - val_categorical_accuracy: 0.1562 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.8028 - categorical_accuracy: 0.6994\n",
      "Epoch 00006: saving model to model_init_2024-09-0212_56_52.893161/model-00006-5.80283-0.69940-13.49101-0.18750.h5\n",
      "21/21 [==============================] - 24s 1s/step - loss: 5.8028 - categorical_accuracy: 0.6994 - val_loss: 13.4910 - val_categorical_accuracy: 0.1875 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.2216 - categorical_accuracy: 0.6875\n",
      "Epoch 00007: saving model to model_init_2024-09-0212_56_52.893161/model-00007-5.22162-0.68750-11.76201-0.21875.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 5.2216 - categorical_accuracy: 0.6875 - val_loss: 11.7620 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.6754 - categorical_accuracy: 0.7604\n",
      "Epoch 00008: saving model to model_init_2024-09-0212_56_52.893161/model-00008-4.67538-0.76042-12.43337-0.21875.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 4.6754 - categorical_accuracy: 0.7604 - val_loss: 12.4334 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.2071 - categorical_accuracy: 0.7857\n",
      "Epoch 00009: saving model to model_init_2024-09-0212_56_52.893161/model-00009-4.20708-0.78571-12.09167-0.23438.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 4.2071 - categorical_accuracy: 0.7857 - val_loss: 12.0917 - val_categorical_accuracy: 0.2344 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.7976 - categorical_accuracy: 0.8036\n",
      "Epoch 00010: saving model to model_init_2024-09-0212_56_52.893161/model-00010-3.79759-0.80357-10.70950-0.26562.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 3.7976 - categorical_accuracy: 0.8036 - val_loss: 10.7095 - val_categorical_accuracy: 0.2656 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb99ce1a970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "Experiment 1: Baseline Conv3D Model summary:\n",
    "loss: 3.7408 - categorical_accuracy: 0.8080 - val_loss: 10.7707 - val_categorical_accuracy: 0.2734 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Increasing number of epochs to 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4957 - categorical_accuracy: 0.9077\n",
      "Epoch 00001: saving model to model_init_2024-09-0212_56_52.893161/model-00001-1.49568-0.90774-7.93262-0.28906.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 1.4957 - categorical_accuracy: 0.9077 - val_loss: 7.9326 - val_categorical_accuracy: 0.2891 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4036 - categorical_accuracy: 0.9182\n",
      "Epoch 00002: saving model to model_init_2024-09-0212_56_52.893161/model-00002-1.40364-0.91815-4.53117-0.42969.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.4036 - categorical_accuracy: 0.9182 - val_loss: 4.5312 - val_categorical_accuracy: 0.4297 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3121 - categorical_accuracy: 0.9301\n",
      "Epoch 00003: saving model to model_init_2024-09-0212_56_52.893161/model-00003-1.31206-0.93006-8.49994-0.28906.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.3121 - categorical_accuracy: 0.9301 - val_loss: 8.4999 - val_categorical_accuracy: 0.2891 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2403 - categorical_accuracy: 0.9301\n",
      "Epoch 00004: saving model to model_init_2024-09-0212_56_52.893161/model-00004-1.24027-0.93006-7.00768-0.35156.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.2403 - categorical_accuracy: 0.9301 - val_loss: 7.0077 - val_categorical_accuracy: 0.3516 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1418 - categorical_accuracy: 0.9390\n",
      "Epoch 00005: saving model to model_init_2024-09-0212_56_52.893161/model-00005-1.14178-0.93899-6.83396-0.27344.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 1.1418 - categorical_accuracy: 0.9390 - val_loss: 6.8340 - val_categorical_accuracy: 0.2734 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1412 - categorical_accuracy: 0.9315\n",
      "Epoch 00006: saving model to model_init_2024-09-0212_56_52.893161/model-00006-1.14116-0.93155-3.82815-0.40625.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.1412 - categorical_accuracy: 0.9315 - val_loss: 3.8282 - val_categorical_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0575 - categorical_accuracy: 0.9405\n",
      "Epoch 00007: saving model to model_init_2024-09-0212_56_52.893161/model-00007-1.05749-0.94048-3.19246-0.49219.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.0575 - categorical_accuracy: 0.9405 - val_loss: 3.1925 - val_categorical_accuracy: 0.4922 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0045 - categorical_accuracy: 0.9405\n",
      "Epoch 00008: saving model to model_init_2024-09-0212_56_52.893161/model-00008-1.00448-0.94048-5.21491-0.36719.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 1.0045 - categorical_accuracy: 0.9405 - val_loss: 5.2149 - val_categorical_accuracy: 0.3672 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9817 - categorical_accuracy: 0.9435\n",
      "Epoch 00009: saving model to model_init_2024-09-0212_56_52.893161/model-00009-0.98172-0.94345-3.92039-0.40625.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.9817 - categorical_accuracy: 0.9435 - val_loss: 3.9204 - val_categorical_accuracy: 0.4062 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9085 - categorical_accuracy: 0.9598\n",
      "Epoch 00010: saving model to model_init_2024-09-0212_56_52.893161/model-00010-0.90848-0.95982-1.99313-0.65625.h5\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.9085 - categorical_accuracy: 0.9598 - val_loss: 1.9931 - val_categorical_accuracy: 0.6562 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9806 - categorical_accuracy: 0.9167\n",
      "Epoch 00011: saving model to model_init_2024-09-0212_56_52.893161/model-00011-0.98064-0.91667-1.80414-0.69531.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.9806 - categorical_accuracy: 0.9167 - val_loss: 1.8041 - val_categorical_accuracy: 0.6953 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9850 - categorical_accuracy: 0.9226\n",
      "Epoch 00012: saving model to model_init_2024-09-0212_56_52.893161/model-00012-0.98502-0.92262-4.21513-0.39062.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.9850 - categorical_accuracy: 0.9226 - val_loss: 4.2151 - val_categorical_accuracy: 0.3906 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9406 - categorical_accuracy: 0.9449\n",
      "Epoch 00013: saving model to model_init_2024-09-0212_56_52.893161/model-00013-0.94064-0.94494-2.99684-0.53906.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.9406 - categorical_accuracy: 0.9449 - val_loss: 2.9968 - val_categorical_accuracy: 0.5391 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8600 - categorical_accuracy: 0.9598\n",
      "Epoch 00014: saving model to model_init_2024-09-0212_56_52.893161/model-00014-0.85998-0.95982-1.85363-0.68750.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.8600 - categorical_accuracy: 0.9598 - val_loss: 1.8536 - val_categorical_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8144 - categorical_accuracy: 0.9613\n",
      "Epoch 00015: saving model to model_init_2024-09-0212_56_52.893161/model-00015-0.81444-0.96131-2.24130-0.60938.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.8144 - categorical_accuracy: 0.9613 - val_loss: 2.2413 - val_categorical_accuracy: 0.6094 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7492 - categorical_accuracy: 0.9688\n",
      "Epoch 00016: saving model to model_init_2024-09-0212_56_52.893161/model-00016-0.74923-0.96875-1.93102-0.64062.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.7492 - categorical_accuracy: 0.9688 - val_loss: 1.9310 - val_categorical_accuracy: 0.6406 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7112 - categorical_accuracy: 0.9583\n",
      "Epoch 00017: saving model to model_init_2024-09-0212_56_52.893161/model-00017-0.71116-0.95833-2.03660-0.60938.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.7112 - categorical_accuracy: 0.9583 - val_loss: 2.0366 - val_categorical_accuracy: 0.6094 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7156 - categorical_accuracy: 0.9598\n",
      "Epoch 00018: saving model to model_init_2024-09-0212_56_52.893161/model-00018-0.71557-0.95982-2.57259-0.57812.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.7156 - categorical_accuracy: 0.9598 - val_loss: 2.5726 - val_categorical_accuracy: 0.5781 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6916 - categorical_accuracy: 0.9643\n",
      "Epoch 00019: saving model to model_init_2024-09-0212_56_52.893161/model-00019-0.69161-0.96429-1.68422-0.67969.h5\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.6916 - categorical_accuracy: 0.9643 - val_loss: 1.6842 - val_categorical_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7421 - categorical_accuracy: 0.9390\n",
      "Epoch 00020: saving model to model_init_2024-09-0212_56_52.893161/model-00020-0.74208-0.93899-1.64550-0.71094.h5\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.7421 - categorical_accuracy: 0.9390 - val_loss: 1.6455 - val_categorical_accuracy: 0.7109 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9a373fd90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "          callbacks=callbacks_list, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "Experiment 2: Increasing number of epochs to 20\n",
    "    loss: 0.7421 - categorical_accuracy: 0.9390 - val_loss: 1.6455 - val_categorical_accuracy: 0.7109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 3: Decreasing batch size from 32 to 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_3 (Conv3D)           (None, 14, 50, 50, 16)    1312      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 14, 50, 50, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 50, 50, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 7, 25, 25, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7, 25, 25, 16)     0         \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 7, 25, 25, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 3, 12, 12, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 3, 12, 12, 32)     0         \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 3, 12, 12, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 1, 6, 6, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1, 6, 6, 64)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,253,701\n",
      "Trainable params: 1,253,477\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(np.arange(0, 40, 3)), 50, 50, 3)\n",
    "model = build_conv3d_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator3 = generator(train_path, train_doc, batch_size)\n",
    "val_generator3 = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_664/3352504766.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator3, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 16\n",
      "Epoch 1/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1369 - categorical_accuracy: 0.8586Source path =  /datasets/Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2024-09-0212_56_52.893161/model-00001-1.13695-0.85863-2.73488-0.58929.h5\n",
      "42/42 [==============================] - 25s 608ms/step - loss: 1.1369 - categorical_accuracy: 0.8586 - val_loss: 2.7349 - val_categorical_accuracy: 0.5893 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.2472 - categorical_accuracy: 0.7574\n",
      "Epoch 00002: saving model to model_init_2024-09-0212_56_52.893161/model-00002-2.24723-0.75744-3.83762-0.56250.h5\n",
      "42/42 [==============================] - 25s 613ms/step - loss: 2.2472 - categorical_accuracy: 0.7574 - val_loss: 3.8376 - val_categorical_accuracy: 0.5625 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.6151 - categorical_accuracy: 0.7902\n",
      "Epoch 00003: saving model to model_init_2024-09-0212_56_52.893161/model-00003-2.61510-0.79018-4.39074-0.60714.h5\n",
      "42/42 [==============================] - 25s 615ms/step - loss: 2.6151 - categorical_accuracy: 0.7902 - val_loss: 4.3907 - val_categorical_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3046 - categorical_accuracy: 0.8378\n",
      "Epoch 00004: saving model to model_init_2024-09-0212_56_52.893161/model-00004-2.30457-0.83780-3.07862-0.66071.h5\n",
      "42/42 [==============================] - 25s 599ms/step - loss: 2.3046 - categorical_accuracy: 0.8378 - val_loss: 3.0786 - val_categorical_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.9465 - categorical_accuracy: 0.8810\n",
      "Epoch 00005: saving model to model_init_2024-09-0212_56_52.893161/model-00005-1.94649-0.88095-2.10355-0.80357.h5\n",
      "42/42 [==============================] - 25s 616ms/step - loss: 1.9465 - categorical_accuracy: 0.8810 - val_loss: 2.1036 - val_categorical_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6036 - categorical_accuracy: 0.9241\n",
      "Epoch 00006: saving model to model_init_2024-09-0212_56_52.893161/model-00006-1.60360-0.92411-2.17911-0.83036.h5\n",
      "42/42 [==============================] - 25s 603ms/step - loss: 1.6036 - categorical_accuracy: 0.9241 - val_loss: 2.1791 - val_categorical_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.4465 - categorical_accuracy: 0.8988\n",
      "Epoch 00007: saving model to model_init_2024-09-0212_56_52.893161/model-00007-1.44646-0.89881-2.99226-0.63393.h5\n",
      "42/42 [==============================] - 25s 600ms/step - loss: 1.4465 - categorical_accuracy: 0.8988 - val_loss: 2.9923 - val_categorical_accuracy: 0.6339 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3255 - categorical_accuracy: 0.9301\n",
      "Epoch 00008: saving model to model_init_2024-09-0212_56_52.893161/model-00008-1.32554-0.93006-2.39609-0.67857.h5\n",
      "42/42 [==============================] - 25s 618ms/step - loss: 1.3255 - categorical_accuracy: 0.9301 - val_loss: 2.3961 - val_categorical_accuracy: 0.6786 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1735 - categorical_accuracy: 0.9435\n",
      "Epoch 00009: saving model to model_init_2024-09-0212_56_52.893161/model-00009-1.17351-0.94345-2.24339-0.60714.h5\n",
      "42/42 [==============================] - 24s 584ms/step - loss: 1.1735 - categorical_accuracy: 0.9435 - val_loss: 2.2434 - val_categorical_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0941 - categorical_accuracy: 0.9301\n",
      "Epoch 00010: saving model to model_init_2024-09-0212_56_52.893161/model-00010-1.09405-0.93006-2.51148-0.58036.h5\n",
      "42/42 [==============================] - 26s 635ms/step - loss: 1.0941 - categorical_accuracy: 0.9301 - val_loss: 2.5115 - val_categorical_accuracy: 0.5804 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1121 - categorical_accuracy: 0.9315\n",
      "Epoch 00011: saving model to model_init_2024-09-0212_56_52.893161/model-00011-1.11207-0.93155-2.02740-0.66964.h5\n",
      "42/42 [==============================] - 26s 627ms/step - loss: 1.1121 - categorical_accuracy: 0.9315 - val_loss: 2.0274 - val_categorical_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1210 - categorical_accuracy: 0.9286\n",
      "Epoch 00012: saving model to model_init_2024-09-0212_56_52.893161/model-00012-1.12097-0.92857-1.99244-0.64286.h5\n",
      "42/42 [==============================] - 26s 644ms/step - loss: 1.1210 - categorical_accuracy: 0.9286 - val_loss: 1.9924 - val_categorical_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1615 - categorical_accuracy: 0.9211\n",
      "Epoch 00013: saving model to model_init_2024-09-0212_56_52.893161/model-00013-1.16155-0.92113-1.97775-0.70536.h5\n",
      "42/42 [==============================] - 25s 620ms/step - loss: 1.1615 - categorical_accuracy: 0.9211 - val_loss: 1.9778 - val_categorical_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2404 - categorical_accuracy: 0.9122\n",
      "Epoch 00014: saving model to model_init_2024-09-0212_56_52.893161/model-00014-1.24039-0.91220-2.15238-0.68750.h5\n",
      "42/42 [==============================] - 24s 597ms/step - loss: 1.2404 - categorical_accuracy: 0.9122 - val_loss: 2.1524 - val_categorical_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1626 - categorical_accuracy: 0.9405\n",
      "Epoch 00015: saving model to model_init_2024-09-0212_56_52.893161/model-00015-1.16256-0.94048-2.04198-0.65179.h5\n",
      "42/42 [==============================] - 24s 588ms/step - loss: 1.1626 - categorical_accuracy: 0.9405 - val_loss: 2.0420 - val_categorical_accuracy: 0.6518 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0226 - categorical_accuracy: 0.9598\n",
      "Epoch 00016: saving model to model_init_2024-09-0212_56_52.893161/model-00016-1.02262-0.95982-1.59280-0.75893.h5\n",
      "42/42 [==============================] - 25s 610ms/step - loss: 1.0226 - categorical_accuracy: 0.9598 - val_loss: 1.5928 - val_categorical_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9191 - categorical_accuracy: 0.9658\n",
      "Epoch 00017: saving model to model_init_2024-09-0212_56_52.893161/model-00017-0.91913-0.96577-1.63674-0.77679.h5\n",
      "42/42 [==============================] - 24s 590ms/step - loss: 0.9191 - categorical_accuracy: 0.9658 - val_loss: 1.6367 - val_categorical_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8976 - categorical_accuracy: 0.9628\n",
      "Epoch 00018: saving model to model_init_2024-09-0212_56_52.893161/model-00018-0.89757-0.96280-1.83791-0.66964.h5\n",
      "42/42 [==============================] - 25s 598ms/step - loss: 0.8976 - categorical_accuracy: 0.9628 - val_loss: 1.8379 - val_categorical_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8590 - categorical_accuracy: 0.9613\n",
      "Epoch 00019: saving model to model_init_2024-09-0212_56_52.893161/model-00019-0.85898-0.96131-1.41987-0.81250.h5\n",
      "42/42 [==============================] - 25s 620ms/step - loss: 0.8590 - categorical_accuracy: 0.9613 - val_loss: 1.4199 - val_categorical_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7450 - categorical_accuracy: 0.9792\n",
      "Epoch 00020: saving model to model_init_2024-09-0212_56_52.893161/model-00020-0.74503-0.97917-1.33624-0.80357.h5\n",
      "42/42 [==============================] - 24s 585ms/step - loss: 0.7450 - categorical_accuracy: 0.9792 - val_loss: 1.3362 - val_categorical_accuracy: 0.8036 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9aaefb8b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator3, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator3, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "    Experiment 3: Decreasing batch size to 16\n",
    "    loss: 0.7450 - categorical_accuracy: 0.9792 - val_loss: 1.3362 - val_categorical_accuracy: 0.8036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-4: Building ConvLSTM model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  ConvLSTM2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build ConvLSTM model\n",
    "def build_convlstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True, input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(ConvLSTM2D(64, (3, 3), padding='same', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimiser = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 14, 50, 50, 32)    40448     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 14, 50, 50, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 50, 50, 64)        221440    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 50, 50, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 160000)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               81920512  \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,185,349\n",
      "Trainable params: 82,185,157\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(np.arange(0, 40, 3)), 50, 50, 3)\n",
    "model4 = build_convlstm_model(input_shape)\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_generator4 = generator(train_path, train_doc, batch_size)\n",
    "val_generator4 = generator(val_path, val_doc, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 43.5839 - categorical_accuracy: 0.3356Source path =  /datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_init_2024-09-0212_56_52.893161/model-00001-43.58387-0.33557-34.74380-0.18304.h5\n",
      "42/42 [==============================] - 57s 1s/step - loss: 43.5839 - categorical_accuracy: 0.3356 - val_loss: 34.7438 - val_categorical_accuracy: 0.1830 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 27.7915 - categorical_accuracy: 0.3787\n",
      "Epoch 00002: saving model to model_init_2024-09-0212_56_52.893161/model-00002-27.79152-0.37872-23.10341-0.28125.h5\n",
      "42/42 [==============================] - 52s 1s/step - loss: 27.7915 - categorical_accuracy: 0.3787 - val_loss: 23.1034 - val_categorical_accuracy: 0.2812 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 17.6541 - categorical_accuracy: 0.4077\n",
      "Epoch 00003: saving model to model_init_2024-09-0212_56_52.893161/model-00003-17.65413-0.40774-14.68094-0.32589.h5\n",
      "42/42 [==============================] - 52s 1s/step - loss: 17.6541 - categorical_accuracy: 0.4077 - val_loss: 14.6809 - val_categorical_accuracy: 0.3259 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 10.9335 - categorical_accuracy: 0.4516\n",
      "Epoch 00004: saving model to model_init_2024-09-0212_56_52.893161/model-00004-10.93346-0.45164-10.60450-0.31696.h5\n",
      "42/42 [==============================] - 52s 1s/step - loss: 10.9335 - categorical_accuracy: 0.4516 - val_loss: 10.6045 - val_categorical_accuracy: 0.3170 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 7.3251 - categorical_accuracy: 0.4688\n",
      "Epoch 00005: saving model to model_init_2024-09-0212_56_52.893161/model-00005-7.32511-0.46875-7.09164-0.44196.h5\n",
      "42/42 [==============================] - 51s 1s/step - loss: 7.3251 - categorical_accuracy: 0.4688 - val_loss: 7.0916 - val_categorical_accuracy: 0.4420 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 5.2978 - categorical_accuracy: 0.4643\n",
      "Epoch 00006: saving model to model_init_2024-09-0212_56_52.893161/model-00006-5.29781-0.46429-5.10902-0.42411.h5\n",
      "42/42 [==============================] - 51s 1s/step - loss: 5.2978 - categorical_accuracy: 0.4643 - val_loss: 5.1090 - val_categorical_accuracy: 0.4241 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.7869 - categorical_accuracy: 0.4926\n",
      "Epoch 00007: saving model to model_init_2024-09-0212_56_52.893161/model-00007-3.78689-0.49256-3.92036-0.40179.h5\n",
      "42/42 [==============================] - 52s 1s/step - loss: 3.7869 - categorical_accuracy: 0.4926 - val_loss: 3.9204 - val_categorical_accuracy: 0.4018 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.5306 - categorical_accuracy: 0.5141\n",
      "Epoch 00008: saving model to model_init_2024-09-0212_56_52.893161/model-00008-3.53060-0.51414-3.84907-0.45982.h5\n",
      "42/42 [==============================] - 52s 1s/step - loss: 3.5306 - categorical_accuracy: 0.5141 - val_loss: 3.8491 - val_categorical_accuracy: 0.4598 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.7351 - categorical_accuracy: 0.5394\n",
      "Epoch 00009: saving model to model_init_2024-09-0212_56_52.893161/model-00009-3.73514-0.53943-4.50682-0.42857.h5\n",
      "42/42 [==============================] - 51s 1s/step - loss: 3.7351 - categorical_accuracy: 0.5394 - val_loss: 4.5068 - val_categorical_accuracy: 0.4286 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.6389 - categorical_accuracy: 0.4978\n",
      "Epoch 00010: saving model to model_init_2024-09-0212_56_52.893161/model-00010-3.63893-0.49777-3.55804-0.60268.h5\n",
      "42/42 [==============================] - 52s 1s/step - loss: 3.6389 - categorical_accuracy: 0.4978 - val_loss: 3.5580 - val_categorical_accuracy: 0.6027 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9ac75e820>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(train_generator4, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "          callbacks=callbacks_list, validation_data=val_generator4, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Experiment 4: Building ConvLSTM model\n",
    " loss: 3.6389 - categorical_accuracy: 0.4978 - val_loss: 3.5580 - val_categorical_accuracy: 0.6027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-5: ConvLSTM model: Increasing num of epochs to 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.1382 - categorical_accuracy: 0.7946\n",
      "Epoch 00001: saving model to model_init_2024-09-0212_56_52.893161/model-00001-2.13821-0.79464-2.84785-0.51786.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't write data (file write failed: time = Mon Sep  2 14:16:18 2024\n, filename = 'model_init_2024-09-0212_56_52.893161/model-00001-2.13821-0.79464-2.84785-0.51786.h5', file descriptor = 86, errno = 28, error message = 'No space left on device', buf = 0x7fb208f06b70, total write size = 327644320, bytes this sub-write = 327644320, bytes actually written = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_664/481318186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model4.fit(train_generator4, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n\u001b[0m\u001b[1;32m      2\u001b[0m           callbacks=callbacks_list, validation_data=val_generator4, validation_steps=validation_steps)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't write data (file write failed: time = Mon Sep  2 14:16:18 2024\n, filename = 'model_init_2024-09-0212_56_52.893161/model-00001-2.13821-0.79464-2.84785-0.51786.h5', file descriptor = 86, errno = 28, error message = 'No space left on device', buf = 0x7fb208f06b70, total write size = 327644320, bytes this sub-write = 327644320, bytes actually written = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "model4.fit(train_generator4, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "          callbacks=callbacks_list, validation_data=val_generator4, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Experiment-5: ConvLSTM model: Increasing num of epochs to 20\n",
    "Not able to complete due to disk error, tried deleting the files and still ending up with same error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 6: CNN + RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + LSTM Model\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN Layers\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'), input_shape=input_shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    # Flatten the output and feed into LSTM\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    # LSTM Layers\n",
    "    model.add(LSTM(256, return_sequences=False, dropout=0.5, recurrent_dropout=0.5))\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimiser = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_shape = (10, 64, 64, 3)  # Example shape (10 frames, 64x64 pixels, 3 channels)\n",
    "batch_size = 32\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_13 (TimeDi  (None, 10, 64, 64, 32)   896       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 10, 64, 64, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 10, 32, 32, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_16 (TimeDi  (None, 10, 32, 32, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 10, 32, 32, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 10, 32, 32, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 10, 16, 16, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 10, 16, 16, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 10, 16, 16, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 10, 16, 16, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 10, 8, 8, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 10, 8, 8, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 10, 8192)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               8651776   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,880,069\n",
      "Trainable params: 8,879,621\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "model = build_cnn_lstm_model(input_shape)\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for learning rate reduction and early stopping\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix for disk memory issue to only save the model with best accuracy\n",
    "\n",
    "# Checkpoints to save the best model\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Callbacks\n",
    "callbacks_list = [checkpoint, LR, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2385 - categorical_accuracy: 1.0000\n",
      "Epoch 00001: val_loss improved from 0.95903 to 0.95001, saving model to best_model.h5\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.2385 - categorical_accuracy: 1.0000 - val_loss: 0.9500 - val_categorical_accuracy: 0.8125 - lr: 2.0000e-04\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2229 - categorical_accuracy: 0.9985\n",
      "Epoch 00002: val_loss improved from 0.95001 to 0.87061, saving model to best_model.h5\n",
      "42/42 [==============================] - 47s 1s/step - loss: 0.2229 - categorical_accuracy: 0.9985 - val_loss: 0.8706 - val_categorical_accuracy: 0.8170 - lr: 2.0000e-04\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2046 - categorical_accuracy: 0.9985\n",
      "Epoch 00003: val_loss did not improve from 0.87061\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.2046 - categorical_accuracy: 0.9985 - val_loss: 0.9199 - val_categorical_accuracy: 0.7902 - lr: 2.0000e-04\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1908 - categorical_accuracy: 0.9985\n",
      "Epoch 00004: val_loss improved from 0.87061 to 0.78364, saving model to best_model.h5\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1908 - categorical_accuracy: 0.9985 - val_loss: 0.7836 - val_categorical_accuracy: 0.8214 - lr: 2.0000e-04\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1767 - categorical_accuracy: 0.9993\n",
      "Epoch 00005: val_loss improved from 0.78364 to 0.72571, saving model to best_model.h5\n",
      "42/42 [==============================] - 49s 1s/step - loss: 0.1767 - categorical_accuracy: 0.9993 - val_loss: 0.7257 - val_categorical_accuracy: 0.8393 - lr: 2.0000e-04\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1634 - categorical_accuracy: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1634 - categorical_accuracy: 1.0000 - val_loss: 0.7943 - val_categorical_accuracy: 0.8080 - lr: 2.0000e-04\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1551 - categorical_accuracy: 0.9978\n",
      "Epoch 00007: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1551 - categorical_accuracy: 0.9978 - val_loss: 0.8263 - val_categorical_accuracy: 0.8170 - lr: 2.0000e-04\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1441 - categorical_accuracy: 0.9978\n",
      "Epoch 00008: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1441 - categorical_accuracy: 0.9978 - val_loss: 0.8442 - val_categorical_accuracy: 0.7812 - lr: 2.0000e-04\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1335 - categorical_accuracy: 0.9993\n",
      "Epoch 00009: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 47s 1s/step - loss: 0.1335 - categorical_accuracy: 0.9993 - val_loss: 0.7943 - val_categorical_accuracy: 0.7812 - lr: 2.0000e-04\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1288 - categorical_accuracy: 0.9985\n",
      "Epoch 00010: val_loss did not improve from 0.72571\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "42/42 [==============================] - 47s 1s/step - loss: 0.1288 - categorical_accuracy: 0.9985 - val_loss: 0.8527 - val_categorical_accuracy: 0.8259 - lr: 2.0000e-04\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1207 - categorical_accuracy: 0.9993\n",
      "Epoch 00011: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 47s 1s/step - loss: 0.1207 - categorical_accuracy: 0.9993 - val_loss: 0.8362 - val_categorical_accuracy: 0.7946 - lr: 4.0000e-05\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1190 - categorical_accuracy: 0.9993\n",
      "Epoch 00012: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1190 - categorical_accuracy: 0.9993 - val_loss: 0.8122 - val_categorical_accuracy: 0.7991 - lr: 4.0000e-05\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1174 - categorical_accuracy: 0.9993\n",
      "Epoch 00013: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1174 - categorical_accuracy: 0.9993 - val_loss: 0.8235 - val_categorical_accuracy: 0.8036 - lr: 4.0000e-05\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1156 - categorical_accuracy: 0.9993\n",
      "Epoch 00014: val_loss did not improve from 0.72571\n",
      "42/42 [==============================] - 49s 1s/step - loss: 0.1156 - categorical_accuracy: 0.9993 - val_loss: 0.7632 - val_categorical_accuracy: 0.8125 - lr: 4.0000e-05\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1102 - categorical_accuracy: 1.0000\n",
      "Epoch 00015: val_loss improved from 0.72571 to 0.70808, saving model to best_model.h5\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1102 - categorical_accuracy: 1.0000 - val_loss: 0.7081 - val_categorical_accuracy: 0.8259 - lr: 4.0000e-05\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1111 - categorical_accuracy: 0.9993\n",
      "Epoch 00016: val_loss did not improve from 0.70808\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1111 - categorical_accuracy: 0.9993 - val_loss: 0.7596 - val_categorical_accuracy: 0.8036 - lr: 4.0000e-05\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1081 - categorical_accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.70808\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1081 - categorical_accuracy: 1.0000 - val_loss: 0.7480 - val_categorical_accuracy: 0.8036 - lr: 4.0000e-05\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1092 - categorical_accuracy: 0.9978\n",
      "Epoch 00018: val_loss improved from 0.70808 to 0.70265, saving model to best_model.h5\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1092 - categorical_accuracy: 0.9978 - val_loss: 0.7027 - val_categorical_accuracy: 0.8304 - lr: 4.0000e-05\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1048 - categorical_accuracy: 0.9993\n",
      "Epoch 00019: val_loss improved from 0.70265 to 0.70251, saving model to best_model.h5\n",
      "42/42 [==============================] - 50s 1s/step - loss: 0.1048 - categorical_accuracy: 0.9993 - val_loss: 0.7025 - val_categorical_accuracy: 0.8348 - lr: 4.0000e-05\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1021 - categorical_accuracy: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.70251\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.1021 - categorical_accuracy: 1.0000 - val_loss: 0.7183 - val_categorical_accuracy: 0.8170 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Experiment-6: CNN + RNN model: \n",
    "loss: 0.1021 - categorical_accuracy: 1.0000 - val_loss: 0.7183 - val_categorical_accuracy: 0.8170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Validation Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
